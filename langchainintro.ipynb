{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQGSfwR00YCuGXVfIfHJUk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","**LangChain** is a powerful framework designed to simplify the integration of language models (like OpenAI’s GPT or other large language models) into applications. It offers tools for managing complex workflows that involve interacting with these models, chaining operations together, and integrating external APIs or tools. LangChain is particularly suited for building applications that require reasoning, decision-making, or multiple steps of interaction with different models or systems.\n","\n","**Core Features of LangChain**\n","\n","**Prompt Templates**\n","These allow developers to structure how they interact with language models. Instead of writing raw prompts, you can define reusable templates that take dynamic inputs, making it easier to create consistent interactions.\n","\n","**Chains**\n","Chains let you combine multiple steps, such as invoking different models, passing outputs from one step as inputs to the next, or interacting with external APIs. LangChain is excellent for workflows that require multi-step processing, such as summarization followed by sentiment analysis.\n","\n","**Agents**\n","Agents are more advanced structures in LangChain, enabling dynamic decision-making. Instead of following a fixed sequence of operations, agents can choose which tool or model to use based on the input. For example, an agent might decide to call a search engine first before processing the results with a language model.\n","\n","**Tools Integration**\n","LangChain integrates with various tools such as search engines, calculators, databases, and APIs. This integration allows for building applications that rely on external data sources or specialized functions that go beyond pure language model outputs.\n","\n","**Memory**\n","LangChain also supports memory capabilities, allowing models to remember previous interactions. This is crucial for creating conversational agents that can maintain context over multiple turns, enabling a more fluid and natural interaction.\n","\n"],"metadata":{"id":"LCTMRQAud9tX"}},{"cell_type":"markdown","source":["Here’s a detailed lecture on key concepts in LangChain with links to relevant documentation for each:\n","\n","---\n","\n","### **1. Prompt Template**\n","\n","**Definition:**  \n","A Prompt Template is a way to create structured prompts that can adapt based on dynamic inputs. It allows you to define a template that can later be used with different variables to generate specific prompts for a model.\n","\n","**Example:**  \n","Imagine you want to generate prompts for a chatbot that answers questions about fitness. You could define a template like:\n","\n","```python\n","template = \"You are a helpful fitness assistant. Please answer the following question: {question}\"\n","prompt = template.format(question=\"What is a good diet plan?\")\n","\n","template = \"\"\"\n","You are an expert at summarizing text.\n","Summarize the following article in a concise manner:\n","\n","Article: {article_text}\n","\n","Summary:\n","\"\"\"\n","\n","# Create a PromptTemplate with the input variable `article_text`\n","prompt = PromptTemplate(\n","    input_variables=[\"article_text\"],\n","    template=template\n",")\n","```\n","\n","By changing the value of `{question}`, you can generate dynamic prompts without writing a new one each time.\n","\n","**Key Use Cases:**  \n","- Creating consistent prompts with flexible inputs.\n","- Automating the generation of prompts based on external data.\n","\n","\n","---\n","\n","### **2. Zero-Shot Prompting**\n","\n","**Definition:**  \n","In zero-shot prompting, the model is given no specific examples or context of how to respond. You simply provide a direct query or instruction, and the model infers what needs to be done.\n","\n","**Example:**  \n","```python\n","prompt = \"What is the capital of France?\"\n","```\n","\n","Here, the model has no previous examples to rely on but can use its training to respond accurately (\"Paris\").\n","\n","**Key Use Cases:**  \n","- Simple queries where the model's general knowledge is sufficient.\n","- Tasks where no specific context or examples are required.\n","\n","\n","---\n","\n","### **3. Few-Shot Prompting**\n","\n","**Definition:**  \n","Few-shot prompting involves providing the model with a small number of examples within the prompt itself. This helps guide the model toward the desired type of response by giving it context.\n","\n","**Example:**\n","```python\n","prompt = \"\"\"\n","Q: What is 5 + 5?\n","A: 10\n","\n","Classify the following text as either \"Positive\" or \"Negative\":\n","Text: \"I waited for over an hour, and the service was really disappointing.\"\n","\"\"\"\n","```\n","\n","Few-shot prompting is particularly useful when the task is more complex and the model needs examples to understand the format or nature of the desired response.\n","\n","**Key Use Cases:**\n","- Improving model performance on specific tasks with examples.\n","- Providing task-specific context.\n","\n","\n","\n","---\n","\n","### **4. Chains**\n","\n","**Definition:**  \n","Chains allow you to combine multiple steps together. In LangChain, you can create complex workflows by connecting multiple calls to models, APIs, or other tools, with each step's output feeding into the next.\n","\n","**Example:**\n","A chain might involve:\n","1. Generating a summary of an article.\n","2. Asking the model to analyze the summary for sentiment.\n","3. Sending the sentiment to another model or API for further action.\n","\n","```python\n","from langchain import LLMChain, PromptTemplate\n","\n","prompt = PromptTemplate(input_variables=[\"text\"], template=\"Summarize this text: {text}\")\n","llm_chain = LLMChain(prompt=prompt, llm=model)\n","summary = llm_chain.run(\"This is a long article about fitness and health...\")\n","```\n","\n","**Key Use Cases:**\n","- Building multi-step workflows that involve different types of processing.\n","- Orchestrating a series of model calls for a larger task.\n","\n","\n","\n","---\n","\n","### **5. Agents**\n","\n","**Definition:**  \n","Agents are more advanced constructs in LangChain that enable models to make decisions about which actions to take next. Instead of following a predefined sequence, agents have the flexibility to dynamically choose which tool, model, or action to invoke based on the input.\n","\n","**Example:**\n","An agent might:\n","- Decide whether to call a search API or a language model based on the user's question.\n","- Chain different types of operations together, making decisions at each step.\n","\n","```python\n","from langchain.agents import initialize_agent, Tool\n","\n","tools = [\n","    Tool(\n","        name=\"Search\",\n","        func=search_tool,\n","        description=\"Use this tool when you need to search for information.\"\n","    )\n","]\n","\n","agent = initialize_agent(tools=tools, llm=model, agent_type=\"zero-shot-react-description\")\n","response = agent.run(\"What is the tallest mountain in the world?\")\n","```\n","\n","**Key Use Cases:**\n","- When multiple tools or actions are needed in a workflow.\n","- Situations where dynamic decision-making is required.\n","\n","\n","\n","\n"],"metadata":{"id":"5uq1BBXvemv5"}},{"cell_type":"markdown","source":["### Memory in LangChain\n","\n","Memory in LangChain refers to the ability of a language model to retain context over interactions, which is crucial for applications that require continuity and coherence in conversations. Memory allows the model to remember previous user inputs and responses, enabling it to provide contextually relevant answers.\n","\n","#### Key Components of Memory in LangChain\n","\n","1. **Conversation Buffer Memory**:\n","   - This stores the entire conversation history, allowing the model to access past messages and generate responses based on that context. It is useful for maintaining continuity in long dialogues.\n","\n","2. **Conversation Chain**:\n","   - This is an abstraction that manages the flow of conversation, coordinating between user inputs, memory, and the language model. It simplifies the interaction by encapsulating all components involved in the conversation.\n","\n","3. **Conversation Buffer Window**:\n","   - Unlike Conversation Buffer Memory, which retains the full history, this only keeps a limited segment of the most recent interactions. This helps optimize memory usage while still providing context from recent exchanges.\n","\n","### Example Code\n","\n","\n","```python\n","from langchain.chains import ConversationChain\n","from langchain.memory import ConversationBufferMemory\n","from langchain.llms import OpenAI\n","\n","# Initialize the language model\n","llm = OpenAI(model_name=\"gpt-3.5-turbo\")\n","\n","# Create a memory instance for storing conversation history\n","memory = ConversationBufferMemory()\n","\n","# Initialize the conversation chain with the language model and memory\n","conversation_chain = ConversationChain(\n","    llm=llm,\n","    memory=memory,\n",")\n","\n","# Simulate a conversation\n","user_input_1 = \"Hello! What can you tell me about LangChain?\"\n","response_1 = conversation_chain({\"input\": user_input_1})\n","print(\"User:\", user_input_1)\n","print(\"Bot:\", response_1['output'])\n","\n","user_input_2 = \"Can it manage memory effectively?\"\n","response_2 = conversation_chain({\"input\": user_input_2})\n","print(\"User:\", user_input_2)\n","print(\"Bot:\", response_2['output'])\n","\n","user_input_3 = \"What about its functionality with external APIs?\"\n","response_3 = conversation_chain({\"input\": user_input_3})\n","print(\"User:\", user_input_3)\n","print(\"Bot:\", response_3['output'])\n","```\n"],"metadata":{"id":"KVsws7vCpmXQ"}},{"cell_type":"code","source":[],"metadata":{"id":"WOvfQB0Jp1io"},"execution_count":null,"outputs":[]}]}